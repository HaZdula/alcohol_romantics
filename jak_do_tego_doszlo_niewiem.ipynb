{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, recall_score, precision_score, plot_precision_recall_curve, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "#import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "pd.options.display.max_columns = None\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>school_MS</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>address_U</th>\n",
       "      <th>famsize_LE3</th>\n",
       "      <th>Pstatus_T</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Mjob_other</th>\n",
       "      <th>Mjob_services</th>\n",
       "      <th>Mjob_teacher</th>\n",
       "      <th>Fjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>Fjob_services</th>\n",
       "      <th>Fjob_teacher</th>\n",
       "      <th>reason_home</th>\n",
       "      <th>reason_other</th>\n",
       "      <th>reason_reputation</th>\n",
       "      <th>guardian_mother</th>\n",
       "      <th>guardian_other</th>\n",
       "      <th>schoolsup_yes</th>\n",
       "      <th>famsup_yes</th>\n",
       "      <th>paid_yes</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>major_por</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  Medu  Fedu  traveltime  studytime  failures  romantic  famrel  \\\n",
       "0   18     4     4           2          2         0         0       4   \n",
       "1   17     1     1           1          2         0         0       5   \n",
       "2   15     1     1           1          2         3         0       4   \n",
       "3   15     4     2           1          3         0         1       3   \n",
       "4   16     3     3           1          2         0         0       4   \n",
       "\n",
       "   freetime  goout  Dalc  Walc  health  absences  G1  G2  G3  school_MS  \\\n",
       "0         3      4     1     1       3         6   5   6   6          0   \n",
       "1         3      3     1     1       3         4   5   5   6          0   \n",
       "2         3      2     2     3       3        10   7   8  10          0   \n",
       "3         2      2     1     1       5         2  15  14  15          0   \n",
       "4         3      2     1     2       5         4   6  10  10          0   \n",
       "\n",
       "   sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  \\\n",
       "0      0          1            0          0            0           0   \n",
       "1      0          1            0          1            0           0   \n",
       "2      0          1            1          1            0           0   \n",
       "3      0          1            0          1            1           0   \n",
       "4      0          1            0          1            0           1   \n",
       "\n",
       "   Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  \\\n",
       "0              0             0            0           0              0   \n",
       "1              0             0            0           1              0   \n",
       "2              0             0            0           1              0   \n",
       "3              0             0            0           0              1   \n",
       "4              0             0            0           1              0   \n",
       "\n",
       "   Fjob_teacher  reason_home  reason_other  reason_reputation  \\\n",
       "0             1            0             0                  0   \n",
       "1             0            0             0                  0   \n",
       "2             0            0             1                  0   \n",
       "3             0            1             0                  0   \n",
       "4             0            1             0                  0   \n",
       "\n",
       "   guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  \\\n",
       "0                1               0              1           0         0   \n",
       "1                0               0              0           1         0   \n",
       "2                1               0              1           0         1   \n",
       "3                1               0              0           1         1   \n",
       "4                0               0              0           1         1   \n",
       "\n",
       "   activities_yes  nursery_yes  higher_yes  internet_yes  major_por  \n",
       "0               0            1           1             0          0  \n",
       "1               0            0           1             1          0  \n",
       "2               0            1           1             1          0  \n",
       "3               1            1           1             1          0  \n",
       "4               0            1           1             0          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"students-all.csv\")\n",
    "# remove rownames\n",
    "df = df.iloc[:,1:]\n",
    "df['romantic'] = np.where(df['romantic']=='yes', 1, 0)\n",
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = df_dummies.sample(frac =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmodyfikowana fun encodująca dane. Beware: target musi być wynikiem encodingu, jeśli romantic znajduje się jako kolumna do zenkodowania -> \"romantic_yes\"/\"romantic_1\" itp, dlatego lepiej zamienić \"romanitc\" ifelsem ręcznie na 0 i 1 dać jako target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_fast(df,target, encoder = None, test_size = None):\n",
    "    \n",
    "    df = df.sample(frac = 1)\n",
    "        \n",
    "    ret = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        df = df.sample(frac = 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis = 1), df[target], test_size = 0.2)\n",
    "            \n",
    "        model = xgb.XGBClassifier()\n",
    "    \n",
    "    \n",
    "        if encoder:\n",
    "            encoder.fit(X_train, y_train)\n",
    "            X_train = encoder.transform(X_train)\n",
    "\n",
    "        if encoder: X_test = encoder.transform(X_test)\n",
    "\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_pred = model.predict_proba(X_test)[:,1]  # positive preds\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "        \n",
    "        ret.append(auc)\n",
    "    \n",
    "        \n",
    "    # returning mean of predictions and model\n",
    "    return np.array(ret).mean(), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8103167949847982,\n",
       " XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "               importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fast(df_dummies, \"romantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_encoder(encoder, model):\n",
    "    \"\"\"\n",
    "    Funkcja działająca jako pipeline w następujących krokach:\n",
    "    * kodowanie\n",
    "    * dopasowanie\n",
    "    * liczenie rmse i r2\n",
    "    \"\"\"\n",
    "    #logreg = LogisticRegression(*args)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['romantic'], axis = 1),df.romantic, test_size = 0.3, random_state = 66)\n",
    "    \n",
    "    encoder.fit(X_train, y_train)\n",
    "    \n",
    "    X_train = encoder.transform(X_train)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #mse = mean_squared_error(y_test, y_pred)\n",
    "    #rmse = np.sqrt(mse)\n",
    "    #r2 = r2_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'famrel', 'freetime', 'goout', 'Dalc',\n",
    "       'Walc', 'health', 'major']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = [ce.BackwardDifferenceEncoder(cols=columns),\n",
    "ce.BaseNEncoder(cols=columns),\n",
    "ce.BinaryEncoder(cols=columns),\n",
    "ce.CatBoostEncoder(cols=columns),\n",
    "ce.HashingEncoder(cols=columns),\n",
    "ce.HelmertEncoder(cols=columns),\n",
    "ce.JamesSteinEncoder(cols=columns),\n",
    "ce.LeaveOneOutEncoder(cols=columns),\n",
    "ce.MEstimateEncoder(cols=columns),\n",
    "ce.OneHotEncoder(cols=columns),\n",
    "ce.OrdinalEncoder(cols=columns),\n",
    "ce.SumEncoder(cols=columns),\n",
    "ce.PolynomialEncoder(cols=columns),\n",
    "ce.TargetEncoder(cols=columns),\n",
    "ce.WOEEncoder(cols=columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def measure_encodesrs(model):\n",
    "    res = pd.DataFrame()\n",
    "    names = ['BackwardDifferenceEncoder',\n",
    "            'BaseNEncoder',\n",
    "            'BinaryEncoder',\n",
    "            'CatBoostEncoder',\n",
    "             'HashingEncoder',\n",
    "             'HelmertEncoder',\n",
    "             'JamesSteinEncoder',\n",
    "             'LeaveOneOutEncoder',\n",
    "             'MEstimateEncoder',\n",
    "             'OneHotEncoder',\n",
    "             'OrdinalEncoder',\n",
    "             'SumEncoder',\n",
    "             'PolynomialEncoder',\n",
    "             'TargetEncoder',\n",
    "             'WOEEncoder'\n",
    "            ]\n",
    "    for i in range(len(enc)):\n",
    "        auc = measure_encoder(enc[i], model)\n",
    "        res = res.append({\"Name\": names[i], \"auc\": auc}, ignore_index = True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "models = [AdaBoostClassifier(), \n",
    "          BaggingClassifier(), \n",
    "          ExtraTreesClassifier(),\n",
    "          RandomForestClassifier(), \n",
    "          GradientBoostingClassifier(),\n",
    "          XGBClassifier(),\n",
    "          XGBRFClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.547339\n",
      "1                BaseNEncoder  0.554279\n",
      "2               BinaryEncoder  0.554279\n",
      "3             CatBoostEncoder  0.612314\n",
      "4              HashingEncoder  0.508089\n",
      "5              HelmertEncoder  0.589993\n",
      "6           JamesSteinEncoder  0.612314\n",
      "7          LeaveOneOutEncoder  0.612314\n",
      "8            MEstimateEncoder  0.612314\n",
      "9               OneHotEncoder  0.588004\n",
      "10             OrdinalEncoder  0.547339\n",
      "11                 SumEncoder  0.588004\n",
      "12          PolynomialEncoder  0.560732\n",
      "13              TargetEncoder  0.612314\n",
      "14                 WOEEncoder  0.612314\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                  max_features=1.0, max_samples=1.0, n_estimators=10,\n",
      "                  n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                  warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.604270\n",
      "1                BaseNEncoder  0.624602\n",
      "2               BinaryEncoder  0.647940\n",
      "3             CatBoostEncoder  0.674196\n",
      "4              HashingEncoder  0.565682\n",
      "5              HelmertEncoder  0.635034\n",
      "6           JamesSteinEncoder  0.641973\n",
      "7          LeaveOneOutEncoder  0.680605\n",
      "8            MEstimateEncoder  0.696473\n",
      "9               OneHotEncoder  0.606745\n",
      "10             OrdinalEncoder  0.614657\n",
      "11                 SumEncoder  0.643432\n",
      "12          PolynomialEncoder  0.653863\n",
      "13              TargetEncoder  0.675168\n",
      "14                 WOEEncoder  0.630569\n",
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.697976\n",
      "1                BaseNEncoder  0.713357\n",
      "2               BinaryEncoder  0.729226\n",
      "3             CatBoostEncoder  0.728739\n",
      "4              HashingEncoder  0.566080\n",
      "5              HelmertEncoder  0.758487\n",
      "6           JamesSteinEncoder  0.715347\n",
      "7          LeaveOneOutEncoder  0.711855\n",
      "8            MEstimateEncoder  0.717822\n",
      "9               OneHotEncoder  0.735679\n",
      "10             OrdinalEncoder  0.693511\n",
      "11                 SumEncoder  0.750044\n",
      "12          PolynomialEncoder  0.705401\n",
      "13              TargetEncoder  0.713844\n",
      "14                 WOEEncoder  0.714330\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.653333\n",
      "1                BaseNEncoder  0.658283\n",
      "2               BinaryEncoder  0.671676\n",
      "3             CatBoostEncoder  0.673665\n",
      "4              HashingEncoder  0.577970\n",
      "5              HelmertEncoder  0.681091\n",
      "6           JamesSteinEncoder  0.655808\n",
      "7          LeaveOneOutEncoder  0.656294\n",
      "8            MEstimateEncoder  0.676140\n",
      "9               OneHotEncoder  0.705888\n",
      "10             OrdinalEncoder  0.652316\n",
      "11                 SumEncoder  0.693998\n",
      "12          PolynomialEncoder  0.667698\n",
      "13              TargetEncoder  0.646879\n",
      "14                 WOEEncoder  0.682107\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.588932\n",
      "1                BaseNEncoder  0.583982\n",
      "2               BinaryEncoder  0.583982\n",
      "3             CatBoostEncoder  0.658327\n",
      "4              HashingEncoder  0.575009\n",
      "5              HelmertEncoder  0.617707\n",
      "6           JamesSteinEncoder  0.658327\n",
      "7          LeaveOneOutEncoder  0.658327\n",
      "8            MEstimateEncoder  0.658327\n",
      "9               OneHotEncoder  0.602325\n",
      "10             OrdinalEncoder  0.590921\n",
      "11                 SumEncoder  0.606789\n",
      "12          PolynomialEncoder  0.626105\n",
      "13              TargetEncoder  0.658327\n",
      "14                 WOEEncoder  0.658327\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.665797\n",
      "1                BaseNEncoder  0.708451\n",
      "2               BinaryEncoder  0.708451\n",
      "3             CatBoostEncoder  0.681665\n",
      "4              HashingEncoder  0.600866\n",
      "5              HelmertEncoder  0.720341\n",
      "6           JamesSteinEncoder  0.681665\n",
      "7          LeaveOneOutEncoder  0.681665\n",
      "8            MEstimateEncoder  0.681665\n",
      "9               OneHotEncoder  0.725778\n",
      "10             OrdinalEncoder  0.671764\n",
      "11                 SumEncoder  0.725778\n",
      "12          PolynomialEncoder  0.707921\n",
      "13              TargetEncoder  0.681665\n",
      "14                 WOEEncoder  0.681665\n",
      "XGBRFClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "                colsample_bynode=0.8, colsample_bytree=None, gamma=None,\n",
      "                gpu_id=None, importance_type='gain',\n",
      "                interaction_constraints=None, learning_rate=1,\n",
      "                max_delta_step=None, max_depth=None, min_child_weight=None,\n",
      "                missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "                n_jobs=None, num_parallel_tree=None,\n",
      "                objective='binary:logistic', random_state=None, reg_alpha=None,\n",
      "                reg_lambda=1e-05, scale_pos_weight=None, subsample=0.8,\n",
      "                tree_method=None, validate_parameters=False, verbosity=None)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.552157\n",
      "1                BaseNEncoder  0.532355\n",
      "2               BinaryEncoder  0.532355\n",
      "3             CatBoostEncoder  0.544731\n",
      "4              HashingEncoder  0.530366\n",
      "5              HelmertEncoder  0.563561\n",
      "6           JamesSteinEncoder  0.544731\n",
      "7          LeaveOneOutEncoder  0.544731\n",
      "8            MEstimateEncoder  0.544731\n",
      "9               OneHotEncoder  0.560069\n",
      "10             OrdinalEncoder  0.543228\n",
      "11                 SumEncoder  0.561572\n",
      "12          PolynomialEncoder  0.555118\n",
      "13              TargetEncoder  0.544731\n",
      "14                 WOEEncoder  0.544731\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for m in models:\n",
    "    print(m)\n",
    "    print(measure_encodesrs(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_res = []\n",
    "for e in enc:\n",
    "    auc,_, = train_fast(df, 'romantic', e)\n",
    "    auc_res.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BackwardDifferenceEncoder</td>\n",
       "      <td>0.790684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaseNEncoder</td>\n",
       "      <td>0.845012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BinaryEncoder</td>\n",
       "      <td>0.825077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoostEncoder</td>\n",
       "      <td>0.821118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HashingEncoder</td>\n",
       "      <td>0.629896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HelmertEncoder</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JamesSteinEncoder</td>\n",
       "      <td>0.827778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LeaveOneOutEncoder</td>\n",
       "      <td>0.820115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MEstimateEncoder</td>\n",
       "      <td>0.809501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.812437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.811792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SumEncoder</td>\n",
       "      <td>0.798155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PolynomialEncoder</td>\n",
       "      <td>0.842633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TargetEncoder</td>\n",
       "      <td>0.783759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WOEEncoder</td>\n",
       "      <td>0.821069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      encoder       auc\n",
       "0   BackwardDifferenceEncoder  0.790684\n",
       "1                BaseNEncoder  0.845012\n",
       "2               BinaryEncoder  0.825077\n",
       "3             CatBoostEncoder  0.821118\n",
       "4              HashingEncoder  0.629896\n",
       "5              HelmertEncoder  0.840093\n",
       "6           JamesSteinEncoder  0.827778\n",
       "7          LeaveOneOutEncoder  0.820115\n",
       "8            MEstimateEncoder  0.809501\n",
       "9               OneHotEncoder  0.812437\n",
       "10             OrdinalEncoder  0.811792\n",
       "11                 SumEncoder  0.798155\n",
       "12          PolynomialEncoder  0.842633\n",
       "13              TargetEncoder  0.783759\n",
       "14                 WOEEncoder  0.821069"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"encoder\": ['BackwardDifferenceEncoder',\n",
    "            'BaseNEncoder',\n",
    "            'BinaryEncoder',\n",
    "            'CatBoostEncoder',\n",
    "             'HashingEncoder',\n",
    "             'HelmertEncoder',\n",
    "             'JamesSteinEncoder',\n",
    "             'LeaveOneOutEncoder',\n",
    "             'MEstimateEncoder',\n",
    "             'OneHotEncoder',\n",
    "             'OrdinalEncoder',\n",
    "             'SumEncoder',\n",
    "             'PolynomialEncoder',\n",
    "             'TargetEncoder',\n",
    "             'WOEEncoder'], \n",
    "             \"auc\": auc_res})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co tu się dzieje????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OneHotEncoder(cols=columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['romantic'], axis=1),df['romantic'], test_size = 0.2, random_state = 666)\n",
    "\n",
    "X_y_train = X_train.copy()\n",
    "\n",
    "X_y_train['romantic'] = y_train\n",
    "\n",
    "X_test_trans = encoder.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.773663667981406"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc,model = train_fast(X_y_train, 'romantic', encoder = ce.OneHotEncoder(cols=columns))\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_test_trans)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5050213675213675"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_dummies.drop(['romantic'], axis=1),\n",
    "                                                    df_dummies['romantic'], test_size = 0.2, random_state = 666)\n",
    "X_y_train = X_train.copy()\n",
    "X_y_train['romantic'] = y_train\n",
    "#X_test_trans = encoder.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc,model = train_fast(X_y_train, 'romantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7538454048556209"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7061899378179762"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesSearchGrid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV, plots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_dummies.drop(['romantic'], axis=1),\n",
    "                                                    df_dummies['romantic'], test_size = 0.2, random_state = 666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.6922155688622754\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7017964071856287\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7125748502994012\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n",
      "best score: 0.7245508982035929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5, error_score='raise',\n",
       "              estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                      colsample_bylevel=None,\n",
       "                                      colsample_bynode=None,\n",
       "                                      colsample_bytree=None, gamma=None,\n",
       "                                      gpu_id=None, importance_type='gain',\n",
       "                                      interaction_constraints=None,\n",
       "                                      learning_rate=None, max_delta_step=None,\n",
       "                                      max_depth=None, min_child_weight=None,\n",
       "                                      missing=nan, monotone_constraints=None,\n",
       "                                      n_...\n",
       "       2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,\n",
       "       3.9, 4. , 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5. , 5.1,\n",
       "       5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6. , 6.1, 6.2, 6.3, 6.4,\n",
       "       6.5, 6.6, 6.7, 6.8, 6.9, 7. , 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7,\n",
       "       7.8, 7.9, 8. , 8.1, 8.2, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9. ,\n",
       "       9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9]),\n",
       "                             'subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def on_step_xgb(optim_result):\n",
    "    score = xgb_bsc.best_score_\n",
    "    print(\"best score: %s\" % score)\n",
    "    return\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "xgb_params = {\n",
    "                \"max_depth\"         : np.arange(3,100),\n",
    "                \"learning_rate\"     : np.arange(0.001,1,0.001, dtype = \"float64\"),\n",
    "                \"booster\"           : [\"gbtree\",\"gblinear\",\"dart\"],\n",
    "                \"subsample\"         : np.arange(0.1,1,0.1, dtype = \"float64\"),\n",
    "                \"colsample_bytree\"  : np.arange(0.1,1,0.1, dtype = \"float64\"),\n",
    "                \"reg_alpha\"         : np.arange(0,10,0.1, dtype = \"float64\"),\n",
    "                \"reg_lambda\"        : np.arange(0,10,0.1, dtype = \"float64\")\n",
    "}\n",
    "\n",
    "xgb_bsc = BayesSearchCV(xgb_model, xgb_params, n_iter = 100, cv = 5)\n",
    "xgb_bsc.fit(X_train,y_train, callback = on_step_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('booster', 'dart'),\n",
       "             ('colsample_bytree', 0.30000000000000004),\n",
       "             ('learning_rate', 0.39),\n",
       "             ('max_depth', 7),\n",
       "             ('reg_alpha', 0.9),\n",
       "             ('reg_lambda', 1.8),\n",
       "             ('subsample', 0.8)])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_bsc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(max_depth = 7,\n",
    "                              booster = \"dart\",\n",
    "                              colsample_bytree = 0.3,\n",
    "                              learning_rate = 0.39,\n",
    "                              reg_alpha = 0.9,\n",
    "                              reg_lambda = 1.8,\n",
    "                              subsample = 0.8)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "y_prob = xgb_model.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694554362163182"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test,y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
