{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, recall_score, precision_score, plot_precision_recall_curve, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "#import xgboost\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "pd.options.display.max_columns = None\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>school_MS</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>address_U</th>\n",
       "      <th>famsize_LE3</th>\n",
       "      <th>Pstatus_T</th>\n",
       "      <th>Mjob_health</th>\n",
       "      <th>Mjob_other</th>\n",
       "      <th>Mjob_services</th>\n",
       "      <th>Mjob_teacher</th>\n",
       "      <th>Fjob_health</th>\n",
       "      <th>Fjob_other</th>\n",
       "      <th>Fjob_services</th>\n",
       "      <th>Fjob_teacher</th>\n",
       "      <th>reason_home</th>\n",
       "      <th>reason_other</th>\n",
       "      <th>reason_reputation</th>\n",
       "      <th>guardian_mother</th>\n",
       "      <th>guardian_other</th>\n",
       "      <th>schoolsup_yes</th>\n",
       "      <th>famsup_yes</th>\n",
       "      <th>paid_yes</th>\n",
       "      <th>activities_yes</th>\n",
       "      <th>nursery_yes</th>\n",
       "      <th>higher_yes</th>\n",
       "      <th>internet_yes</th>\n",
       "      <th>major_por</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  Medu  Fedu  traveltime  studytime  failures  romantic  famrel  \\\n",
       "0   18     4     4           2          2         0         0       4   \n",
       "1   17     1     1           1          2         0         0       5   \n",
       "2   15     1     1           1          2         3         0       4   \n",
       "3   15     4     2           1          3         0         1       3   \n",
       "4   16     3     3           1          2         0         0       4   \n",
       "\n",
       "   freetime  goout  Dalc  Walc  health  absences  G1  G2  G3  school_MS  \\\n",
       "0         3      4     1     1       3         6   5   6   6          0   \n",
       "1         3      3     1     1       3         4   5   5   6          0   \n",
       "2         3      2     2     3       3        10   7   8  10          0   \n",
       "3         2      2     1     1       5         2  15  14  15          0   \n",
       "4         3      2     1     2       5         4   6  10  10          0   \n",
       "\n",
       "   sex_M  address_U  famsize_LE3  Pstatus_T  Mjob_health  Mjob_other  \\\n",
       "0      0          1            0          0            0           0   \n",
       "1      0          1            0          1            0           0   \n",
       "2      0          1            1          1            0           0   \n",
       "3      0          1            0          1            1           0   \n",
       "4      0          1            0          1            0           1   \n",
       "\n",
       "   Mjob_services  Mjob_teacher  Fjob_health  Fjob_other  Fjob_services  \\\n",
       "0              0             0            0           0              0   \n",
       "1              0             0            0           1              0   \n",
       "2              0             0            0           1              0   \n",
       "3              0             0            0           0              1   \n",
       "4              0             0            0           1              0   \n",
       "\n",
       "   Fjob_teacher  reason_home  reason_other  reason_reputation  \\\n",
       "0             1            0             0                  0   \n",
       "1             0            0             0                  0   \n",
       "2             0            0             1                  0   \n",
       "3             0            1             0                  0   \n",
       "4             0            1             0                  0   \n",
       "\n",
       "   guardian_mother  guardian_other  schoolsup_yes  famsup_yes  paid_yes  \\\n",
       "0                1               0              1           0         0   \n",
       "1                0               0              0           1         0   \n",
       "2                1               0              1           0         1   \n",
       "3                1               0              0           1         1   \n",
       "4                0               0              0           1         1   \n",
       "\n",
       "   activities_yes  nursery_yes  higher_yes  internet_yes  major_por  \n",
       "0               0            1           1             0          0  \n",
       "1               0            0           1             1          0  \n",
       "2               0            1           1             1          0  \n",
       "3               1            1           1             1          0  \n",
       "4               0            1           1             0          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"students-all.csv\")\n",
    "# remove rownames\n",
    "df = df.iloc[:,1:]\n",
    "df['romantic'] = np.where(df['romantic']=='yes', 1, 0)\n",
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = df_dummies.sample(frac =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmodyfikowana fun encodująca dane. Beware: target musi być wynikiem encodingu, jeśli romantic znajduje się jako kolumna do zenkodowania -> \"romantic_yes\"/\"romantic_1\" itp, dlatego lepiej zamienić \"romanitc\" ifelsem ręcznie na 0 i 1 dać jako target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_fast(df,target, encoder = None, test_size = None):\n",
    "    \n",
    "    df = df.sample(frac = 1)\n",
    "        \n",
    "    ret = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        \n",
    "        df = df.sample(frac = 1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis = 1), df[target], test_size = 0.2)\n",
    "            \n",
    "        model = xgb.XGBClassifier()\n",
    "    \n",
    "    \n",
    "        if encoder:\n",
    "            encoder.fit(X_train, y_train)\n",
    "            X_train = encoder.transform(X_train)\n",
    "\n",
    "        if encoder: X_test = encoder.transform(X_test)\n",
    "\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        y_pred = model.predict_proba(X_test)[:,1]  # positive preds\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "        \n",
    "        ret.append(auc)\n",
    "    \n",
    "        \n",
    "    # returning mean of predictions and model\n",
    "    return np.array(ret).mean(), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8240050668355494,\n",
       " XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "               importance_type='gain', interaction_constraints=None,\n",
       "               learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "               min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "               objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "               reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "               validate_parameters=False, verbosity=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fast(df_dummies, \"romantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_encoder(encoder, model):\n",
    "    \"\"\"\n",
    "    Funkcja działająca jako pipeline w następujących krokach:\n",
    "    * kodowanie\n",
    "    * dopasowanie\n",
    "    * liczenie rmse i r2\n",
    "    \"\"\"\n",
    "    #logreg = LogisticRegression(*args)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['romantic'], axis = 1),df.romantic, test_size = 0.3, random_state = 66)\n",
    "    \n",
    "    encoder.fit(X_train, y_train)\n",
    "    \n",
    "    X_train = encoder.transform(X_train)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #mse = mean_squared_error(y_test, y_pred)\n",
    "    #rmse = np.sqrt(mse)\n",
    "    #r2 = r2_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'famrel', 'freetime', 'goout', 'Dalc',\n",
    "       'Walc', 'health', 'major']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = [ce.BackwardDifferenceEncoder(cols=columns),\n",
    "ce.BaseNEncoder(cols=columns),\n",
    "ce.BinaryEncoder(cols=columns),\n",
    "ce.CatBoostEncoder(cols=columns),\n",
    "ce.HashingEncoder(cols=columns),\n",
    "ce.HelmertEncoder(cols=columns),\n",
    "ce.JamesSteinEncoder(cols=columns),\n",
    "ce.LeaveOneOutEncoder(cols=columns),\n",
    "ce.MEstimateEncoder(cols=columns),\n",
    "ce.OneHotEncoder(cols=columns),\n",
    "ce.OrdinalEncoder(cols=columns),\n",
    "ce.SumEncoder(cols=columns),\n",
    "ce.PolynomialEncoder(cols=columns),\n",
    "ce.TargetEncoder(cols=columns),\n",
    "ce.WOEEncoder(cols=columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def measure_encodesrs(model):\n",
    "    res = pd.DataFrame()\n",
    "    names = ['BackwardDifferenceEncoder',\n",
    "            'BaseNEncoder',\n",
    "            'BinaryEncoder',\n",
    "            'CatBoostEncoder',\n",
    "             'HashingEncoder',\n",
    "             'HelmertEncoder',\n",
    "             'JamesSteinEncoder',\n",
    "             'LeaveOneOutEncoder',\n",
    "             'MEstimateEncoder',\n",
    "             'OneHotEncoder',\n",
    "             'OrdinalEncoder',\n",
    "             'SumEncoder',\n",
    "             'PolynomialEncoder',\n",
    "             'TargetEncoder',\n",
    "             'WOEEncoder'\n",
    "            ]\n",
    "    for i in range(len(enc)):\n",
    "        auc = measure_encoder(enc[i], model)\n",
    "        res = res.append({\"Name\": names[i], \"auc\": auc}, ignore_index = True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "models = [AdaBoostClassifier(), \n",
    "          BaggingClassifier(), \n",
    "          ExtraTreesClassifier(),\n",
    "          RandomForestClassifier(), \n",
    "          GradientBoostingClassifier(),\n",
    "          XGBClassifier(),\n",
    "          XGBRFClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.547339\n",
      "1                BaseNEncoder  0.554279\n",
      "2               BinaryEncoder  0.554279\n",
      "3             CatBoostEncoder  0.612314\n",
      "4              HashingEncoder  0.508089\n",
      "5              HelmertEncoder  0.589993\n",
      "6           JamesSteinEncoder  0.612314\n",
      "7          LeaveOneOutEncoder  0.612314\n",
      "8            MEstimateEncoder  0.612314\n",
      "9               OneHotEncoder  0.588004\n",
      "10             OrdinalEncoder  0.547339\n",
      "11                 SumEncoder  0.588004\n",
      "12          PolynomialEncoder  0.560732\n",
      "13              TargetEncoder  0.612314\n",
      "14                 WOEEncoder  0.612314\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
      "                  max_features=1.0, max_samples=1.0, n_estimators=10,\n",
      "                  n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                  warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.606259\n",
      "1                BaseNEncoder  0.653907\n",
      "2               BinaryEncoder  0.607762\n",
      "3             CatBoostEncoder  0.592866\n",
      "4              HashingEncoder  0.604800\n",
      "5              HelmertEncoder  0.605286\n",
      "6           JamesSteinEncoder  0.637995\n",
      "7          LeaveOneOutEncoder  0.624116\n",
      "8            MEstimateEncoder  0.644979\n",
      "9               OneHotEncoder  0.680649\n",
      "10             OrdinalEncoder  0.629066\n",
      "11                 SumEncoder  0.624602\n",
      "12          PolynomialEncoder  0.701954\n",
      "13              TargetEncoder  0.625619\n",
      "14                 WOEEncoder  0.620624\n",
      "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                     criterion='gini', max_depth=None, max_features='auto',\n",
      "                     max_leaf_nodes=None, max_samples=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
      "                     warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.695987\n",
      "1                BaseNEncoder  0.744607\n",
      "2               BinaryEncoder  0.740629\n",
      "3             CatBoostEncoder  0.719811\n",
      "4              HashingEncoder  0.559627\n",
      "5              HelmertEncoder  0.739613\n",
      "6           JamesSteinEncoder  0.743105\n",
      "7          LeaveOneOutEncoder  0.702440\n",
      "8            MEstimateEncoder  0.732717\n",
      "9               OneHotEncoder  0.737182\n",
      "10             OrdinalEncoder  0.693511\n",
      "11                 SumEncoder  0.726264\n",
      "12          PolynomialEncoder  0.709379\n",
      "13              TargetEncoder  0.733690\n",
      "14                 WOEEncoder  0.715833\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.674151\n",
      "1                BaseNEncoder  0.665223\n",
      "2               BinaryEncoder  0.676627\n",
      "3             CatBoostEncoder  0.676140\n",
      "4              HashingEncoder  0.585926\n",
      "5              HelmertEncoder  0.692008\n",
      "6           JamesSteinEncoder  0.698948\n",
      "7          LeaveOneOutEncoder  0.655808\n",
      "8            MEstimateEncoder  0.653819\n",
      "9               OneHotEncoder  0.681091\n",
      "10             OrdinalEncoder  0.660758\n",
      "11                 SumEncoder  0.709379\n",
      "12          PolynomialEncoder  0.696473\n",
      "13              TargetEncoder  0.642415\n",
      "14                 WOEEncoder  0.680118\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.586457\n",
      "1                BaseNEncoder  0.586457\n",
      "2               BinaryEncoder  0.583982\n",
      "3             CatBoostEncoder  0.658327\n",
      "4              HashingEncoder  0.542786\n",
      "5              HelmertEncoder  0.622657\n",
      "6           JamesSteinEncoder  0.658327\n",
      "7          LeaveOneOutEncoder  0.662792\n",
      "8            MEstimateEncoder  0.662792\n",
      "9               OneHotEncoder  0.609264\n",
      "10             OrdinalEncoder  0.589418\n",
      "11                 SumEncoder  0.609264\n",
      "12          PolynomialEncoder  0.626105\n",
      "13              TargetEncoder  0.662792\n",
      "14                 WOEEncoder  0.658327\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              objective='binary:logistic', random_state=None, reg_alpha=None,\n",
      "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
      "              tree_method=None, validate_parameters=False, verbosity=None)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.665797\n",
      "1                BaseNEncoder  0.708451\n",
      "2               BinaryEncoder  0.708451\n",
      "3             CatBoostEncoder  0.681665\n",
      "4              HashingEncoder  0.600866\n",
      "5              HelmertEncoder  0.720341\n",
      "6           JamesSteinEncoder  0.681665\n",
      "7          LeaveOneOutEncoder  0.681665\n",
      "8            MEstimateEncoder  0.681665\n",
      "9               OneHotEncoder  0.725778\n",
      "10             OrdinalEncoder  0.671764\n",
      "11                 SumEncoder  0.725778\n",
      "12          PolynomialEncoder  0.707921\n",
      "13              TargetEncoder  0.681665\n",
      "14                 WOEEncoder  0.681665\n",
      "XGBRFClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "                colsample_bynode=0.8, colsample_bytree=None, gamma=None,\n",
      "                gpu_id=None, importance_type='gain',\n",
      "                interaction_constraints=None, learning_rate=1,\n",
      "                max_delta_step=None, max_depth=None, min_child_weight=None,\n",
      "                missing=nan, monotone_constraints=None, n_estimators=100,\n",
      "                n_jobs=None, num_parallel_tree=None,\n",
      "                objective='binary:logistic', random_state=None, reg_alpha=None,\n",
      "                reg_lambda=1e-05, scale_pos_weight=None, subsample=0.8,\n",
      "                tree_method=None, validate_parameters=False, verbosity=None)\n",
      "                         Name       auc\n",
      "0   BackwardDifferenceEncoder  0.545704\n",
      "1                BaseNEncoder  0.552687\n",
      "2               BinaryEncoder  0.552687\n",
      "3             CatBoostEncoder  0.559097\n",
      "4              HashingEncoder  0.533327\n",
      "5              HelmertEncoder  0.566036\n",
      "6           JamesSteinEncoder  0.559097\n",
      "7          LeaveOneOutEncoder  0.559097\n",
      "8            MEstimateEncoder  0.559097\n",
      "9               OneHotEncoder  0.557594\n",
      "10             OrdinalEncoder  0.552643\n",
      "11                 SumEncoder  0.553129\n",
      "12          PolynomialEncoder  0.555118\n",
      "13              TargetEncoder  0.559097\n",
      "14                 WOEEncoder  0.559097\n"
     ]
    }
   ],
   "source": [
    "r = []\n",
    "for m in models:\n",
    "    print(m)\n",
    "    print(measure_encodesrs(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_res = []\n",
    "for e in enc:\n",
    "    auc,_, = train_fast(df, 'romantic', e)\n",
    "    auc_res.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BackwardDifferenceEncoder</td>\n",
       "      <td>0.790684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BaseNEncoder</td>\n",
       "      <td>0.845012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BinaryEncoder</td>\n",
       "      <td>0.825077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoostEncoder</td>\n",
       "      <td>0.821118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HashingEncoder</td>\n",
       "      <td>0.629896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HelmertEncoder</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JamesSteinEncoder</td>\n",
       "      <td>0.827778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LeaveOneOutEncoder</td>\n",
       "      <td>0.820115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MEstimateEncoder</td>\n",
       "      <td>0.809501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OneHotEncoder</td>\n",
       "      <td>0.812437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OrdinalEncoder</td>\n",
       "      <td>0.811792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SumEncoder</td>\n",
       "      <td>0.798155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PolynomialEncoder</td>\n",
       "      <td>0.842633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TargetEncoder</td>\n",
       "      <td>0.783759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WOEEncoder</td>\n",
       "      <td>0.821069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      encoder       auc\n",
       "0   BackwardDifferenceEncoder  0.790684\n",
       "1                BaseNEncoder  0.845012\n",
       "2               BinaryEncoder  0.825077\n",
       "3             CatBoostEncoder  0.821118\n",
       "4              HashingEncoder  0.629896\n",
       "5              HelmertEncoder  0.840093\n",
       "6           JamesSteinEncoder  0.827778\n",
       "7          LeaveOneOutEncoder  0.820115\n",
       "8            MEstimateEncoder  0.809501\n",
       "9               OneHotEncoder  0.812437\n",
       "10             OrdinalEncoder  0.811792\n",
       "11                 SumEncoder  0.798155\n",
       "12          PolynomialEncoder  0.842633\n",
       "13              TargetEncoder  0.783759\n",
       "14                 WOEEncoder  0.821069"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"encoder\": ['BackwardDifferenceEncoder',\n",
    "            'BaseNEncoder',\n",
    "            'BinaryEncoder',\n",
    "            'CatBoostEncoder',\n",
    "             'HashingEncoder',\n",
    "             'HelmertEncoder',\n",
    "             'JamesSteinEncoder',\n",
    "             'LeaveOneOutEncoder',\n",
    "             'MEstimateEncoder',\n",
    "             'OneHotEncoder',\n",
    "             'OrdinalEncoder',\n",
    "             'SumEncoder',\n",
    "             'PolynomialEncoder',\n",
    "             'TargetEncoder',\n",
    "             'WOEEncoder'], \n",
    "             \"auc\": auc_res})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
